{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "801aa_xvWTnG",
        "outputId": "a4208cb2-bcf0-4cdd-bdd9-4d55e5c53f56"
      },
      "outputs": [],
      "source": [
        "#script to download data from the google drive\n",
        "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9,fa;q=0.8\" --header=\"Cookie: SID=g.a000gAih6wfkjbcbADSz62FJPjIAu0M9JFQvPctI__Kec1ykJV_0UKFwxW_kYqkWSVnwLNbCdgACgYKAWwSAQASFQHGX2MiPA83fXJNYO5R44lOJHFU7hoVAUF8yKrYo9Jjp_WQrUAO_eP-00570076; __Secure-1PSID=g.a000gAih6wfkjbcbADSz62FJPjIAu0M9JFQvPctI__Kec1ykJV_0EaA3rzo3JlioS0jL98KYPQACgYKAccSAQASFQHGX2Miy8vlaewbNEPEU8dFD-DPNBoVAUF8yKo1GeLaYOqiGsvuYVQmR0Gj0076; __Secure-3PSID=g.a000gAih6wfkjbcbADSz62FJPjIAu0M9JFQvPctI__Kec1ykJV_0daBOqj8Sl6aWULeHb8tTFAACgYKASoSAQASFQHGX2MiU_Z8GO_YZyp2xk9ipkG_qRoVAUF8yKqwUJ6U4fkIpUEvqydw10-C0076; HSID=Abda-Gu80HucPyRdw; SSID=AYc-aPVVRZHjx9zPm; APISID=woK740gzd8Mq8kes/Aad25S2XAz6880nUJ; SAPISID=ULq-o5cD2WNC6_GS/AqprLuiDBmBUYwwbH; __Secure-1PAPISID=ULq-o5cD2WNC6_GS/AqprLuiDBmBUYwwbH; __Secure-3PAPISID=ULq-o5cD2WNC6_GS/AqprLuiDBmBUYwwbH; AEC=Ae3NU9MGppRzqPBz29a_V94PJiE37Upgo3fiOY7BQf6fUQy0fifpTrmNUA; __Secure-ENID=17.SE=mSlGvO_Zx1CoU0XyDrk5A9MsubxTHPSq69XI7GooDN15PPT5s7kXg9Fnnp9MhlE7jiJXQgKeD95KnKc7TDO7DIdGebg4F9fT-IO4KVAcsPmWo0ASzhd4e_tgvfR8PungZt9jURyXcjUDpoHJNdF-Ola5RdaKaCAm0pfoZIxIX-VQ2lJ3WOXmhoxKoyF6OcAaDUQGqTrXCG4Ul2uLZe1PVf9Duui7gpnLfKBwsgkaSdEYTK8kBeKc06y9K6HLvvyUhKEiD9GLLrjRIP6e51hgg9aeLUGcD8Hk-A4o961nJi8rHfPALwgkkhvSewckxgzBXjmkzOnIFeW4vzwfNR6TpZp-cC0tZ0oeOIWJjF7TtIH1sm_siprks0spbE76QiKtMJrwo61OGR2PJok; AMP_MKTG_630874745a=JTdCJTdE; AMP_630874745a=JTdCJTIyZGV2aWNlSWQlMjIlM0ElMjI0ODU1MGQ2NC1kMmM2LTQwNjMtYmQyMy1mNjM2YTg1NDkxOGIlMjIlMkMlMjJ1c2VySWQlMjIlM0ElMjJobTIxMDBtaCU0MGdtYWlsLmNvbSUyMiUyQyUyMnNlc3Npb25JZCUyMiUzQTE3MDc2NDk3MjE3MjElMkMlMjJvcHRPdXQlMjIlM0FmYWxzZSUyQyUyMmxhc3RFdmVudFRpbWUlMjIlM0ExNzA3NjQ5NzIxOTMyJTJDJTIybGFzdEV2ZW50SWQlMjIlM0EzJTdE; NID=511=YT1iftDi-fBjdwnvc3CoI6x5mPzJbtEWlZf1jrTWPISJQztb-YbJcuRp3X7f6ZolaO_l29b_unlLbZ8vNFZKGM2Q2aAnYrLVDzwM5ftxqX7ThxCVQ6cQrWqIT-CwxkETn4ivK1_rKBlPL_oLTX96W72LLqjFVUnJNf_XXREZ-oEzT8-XHjPgdTaCP1BqeH36N8RS2wqsXvIgHD60tSJ0OXtdygH_2AgKi9eVZxTmJQSw2MRNoTSVDYl2LmRb_liylBHzblVdLP4kJ0Kjj3_v8QOIKZRg8Dd79yu3KHbWU0Dd2lbCsuq79t0tdmCVUiVr2odUxoJ1WMya82QNJEJ_zWLKrA2EbaeVtqVcPJUbKsWJA5OVJKcNIUXnP0CCIA7y05uMTDiQojryVmQJz58kgr7XgxyknnXpOiTJRlc_rqurtUOf9BOX4IJVhT_9PkWD4If9cl4n8ZMCY5JA4tz15bEzEuhY0RlammGJR-7d0nbRnm9fCofANemqIj-m6dDC2jP2_Hs7jWmnSphH4He80GoCFHwpifjXvLDEe9NN_haWutR3rtjua0aAFryK09cEAROYSFgfmfawe2E10jV6iXKlwF1gzLma9jUTiOrgsC49VLGTGFcSh-8C0nO7QWICsup4DRug89wSoNzbxe-VkViVLXptK7DCU8REv4SCujqhoexC50WHD197hV73vlBa3vq_NXqc; __Secure-1PSIDTS=sidts-CjEBPVxjSiPBKn40DYo4nGcMQp1z758lZH6I_Hc-ZgXqILnbgFHs3tqG8vn_bv9zUhrLEAA; __Secure-3PSIDTS=sidts-CjEBPVxjSiPBKn40DYo4nGcMQp1z758lZH6I_Hc-ZgXqILnbgFHs3tqG8vn_bv9zUhrLEAA; ph_phc_cjuWI21zkiw3B30S2vdrLd2HtfmqRWuzZMldJrXEpPw_posthog=%7B%22distinct_id%22%3A%2218d987a092756c-0cf9e32b2aae19-1e525637-fa000-18d987a0928f50%22%2C%22%24device_id%22%3A%2218d987a092756c-0cf9e32b2aae19-1e525637-fa000-18d987a0928f50%22%2C%22%24user_state%22%3A%22anonymous%22%2C%22%24referrer%22%3A%22%24direct%22%2C%22%24referring_domain%22%3A%22%24direct%22%2C%22%24sesid%22%3A%5B1707660162905%2C%2218d987a092f2e7-0cd80a666118e5-1e525637-fa000-18d987a093036f%22%2C1707660151087%5D%2C%22%24session_recording_enabled_server_side%22%3Afalse%2C%22%24active_feature_flags%22%3A%5B%5D%2C%22%24enabled_feature_flags%22%3A%7B%7D%2C%22%24feature_flag_payloads%22%3A%7B%7D%7D; 1P_JAR=2024-02-11-14; mp_851392464b60e8cc1948a193642f793b_mixpanel=%7B%22distinct_id%22%3A%20%22%24device%3A18d9774c207714-093e713ddbba6f-1e525637-fa000-18d9774c207714%22%2C%22%24device_id%22%3A%20%2218d9774c207714-093e713ddbba6f-1e525637-fa000-18d9774c207714%22%2C%22%24initial_referrer%22%3A%20%22%24direct%22%2C%22%24initial_referring_domain%22%3A%20%22%24direct%22%7D; SIDCC=ABTWhQGUo_c7QMzJlJ7B3MDwdlxCSzVEGzdnCZ2vTNYf7jPYvq5f87wepPlk8GZSZi-XzOJ_HRKQ;__Secure-1PSIDCC=ABTWhQHvLXW1lnu41w0U9AXYAOP4JPcogay1dxjKeHVVrg6S4wHEoj-nuMfcIOQBc6beobZei1I; __Secure-3PSIDCC=ABTWhQFw_jZxYIo4ZvYRabvxKWJg3MBGwNQ0i5FSiV-tskZHup5Y9QKUAScdVHFFZNrYcPYFZI0\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1P4E6cTn4X6rgK9SyOsMA344kLNyISPHQ&export=download&authuser=0&confirm=t&uuid=cd6bed3e-4a7c-47ba-a27e-b581a55f3623&at=APZUnTVMN7FsFLBDI4_D7CAJCR7d%3A1707660525023\" -c -O 'Phase1.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4uMifr__aDc"
      },
      "source": [
        "#### **1.1. Import Required Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58da9001"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
        "from keras.layers import Input, Add, ZeroPadding2D, BatchNormalization, Conv2D, AveragePooling2D, GlobalMaxPooling2D\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6tmATL4_ecG"
      },
      "source": [
        "#### **1.2. Test-Train Data**\n",
        "**Split the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "907a8041"
      },
      "outputs": [],
      "source": [
        "source_dir = '/content/Train'\n",
        "dest_dir = '/content/Test'\n",
        "\n",
        "os.makedirs(os.path.join(dest_dir, 'OK'), exist_ok=True)\n",
        "os.makedirs(os.path.join(dest_dir, 'Defected'), exist_ok=True)\n",
        "\n",
        "for category in ['OK', 'Defected']:\n",
        "    source_dir = os.path.join(source_dir, category)\n",
        "    dest_dir = os.path.join(dest_dir, category)\n",
        "\n",
        "    images = os.listdir(source_dir)\n",
        "    num_images_to_move = int(0.15 * len(images))  # Randomly select 15% of images to test\n",
        "    images_to_move = random.sample(images, num_images_to_move)\n",
        "\n",
        "    # Move the selected images to Test Images directory\n",
        "    for image in images_to_move:\n",
        "        source_path = os.path.join(source_dir, image)\n",
        "        dest_path = os.path.join(dest_dir, image)\n",
        "        shutil.move(source_path, dest_path)\n",
        "\n",
        "print(\"Images spilted successfully!\")\n",
        "\n",
        "def get_files(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    return 0\n",
        "  count=0\n",
        "  # crawls inside folders\n",
        "  for current_path,dirs,files in os.walk(directory):\n",
        "    for dr in dirs:\n",
        "      count+= len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n",
        "  return count\n",
        "train_dir =\"/content/Train\"\n",
        "test_dir=\"/content/Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07bcaa8",
        "outputId": "2501f6dc-83e1-49c1-8b0c-4dd5956c2ef5"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_samples =get_files(train_dir)     #train file image count\n",
        "\n",
        "num_classes=len(glob.glob(train_dir+\"/*\"))\n",
        "\n",
        "test_samples=get_files(test_dir)  #test file image count\n",
        "print(num_classes,\"Classes\")\n",
        "print(train_samples,\"Train images\")\n",
        "print(test_samples,\"Test images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuGK0vnQ_jRe"
      },
      "source": [
        "#### **1.3. ImageDataGenerator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa4DsLKmxl6w"
      },
      "source": [
        "**ImageDataGenerator**,Data augmentation is used to increase the size of training set. Through Data augmentation we want to prevent overfitting ,this refers to randomly changing the images in ways that shouldnâ€™t impact their interpretation, such as horizontal flipping, zooming, and rotating\n",
        "* **Rescale:** One of the many magnification parameters adjusts the pixel values of our image.\n",
        "* **Shear_range:** counterclockwise shear angle in degrees\n",
        "* **Zoom_range:** zoom\n",
        "* **Horizontal_flip:** flip image horizontally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3d73be1"
      },
      "outputs": [],
      "source": [
        "train_datagen=ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        "    )\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U7ZGKPT8ChD"
      },
      "source": [
        "**flow_from_directory() -->** Another method to read images into TensorFlow environment is to use the .\n",
        "\n",
        "**Parameters:**\n",
        "* **directory:** The path of the target directory. It must contain one subdirectory per class. Any PNG, JPG, BMP or TIF formatted images found in each of the subdirectories will be included in the generator.\n",
        "* **target_size:** A tuple of integers, (height, width), by default (256,256). All found images will be resized.\n",
        "* **batch_size:** The size of the data chunks (default: 32).\n",
        "* **shuffle:** Decides whether to shuffle data (default: True). If set to false, it sorts the data in alphanumeric order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwD6Vmh4mrDl",
        "outputId": "f6e6b17b-d050-4a25-c2d1-6d3e91c69494"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def resize_images(folder_path, target_size=(224, 224)):  #resizing fusnction\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            img = Image.open(file_path)\n",
        "            img_resized = img.resize(target_size, Image.ANTIALIAS)\n",
        "            img_resized.save(file_path)\n",
        "\n",
        "#Defected Casting paths for the Train and Test directories\n",
        "train_dir = '/content/Train/Defected'\n",
        "test_dir = '/content/Test/Defected'\n",
        "resize_images(train_dir)\n",
        "resize_images(test_dir)\n",
        "\n",
        "#Non defected Casting paths for the Train and Test directories\n",
        "train_dir = '/content/Train/OK'\n",
        "test_dir = '/content/Test/OK'\n",
        "resize_images(train_dir)\n",
        "resize_images(test_dir)\n",
        "\n",
        "print(\"Images resized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WpHBoue9d3M",
        "outputId": "49186b2b-03bc-4c18-d72b-318b0b873a04"
      },
      "outputs": [],
      "source": [
        "checkpoint_folder = os.path.join('/content/Train', \".ipynb_checkpoints\")\n",
        "\n",
        "if os.path.exists(checkpoint_folder):\n",
        "    os.rmdir(checkpoint_folder)\n",
        "    print(f\"Deleted .ipynb_checkpoints folder successfully.\")\n",
        "else:\n",
        "    print(\".ipynb_checkpoints folder does not exist.\")\n",
        "    \n",
        "input_shape=(224,224,3)\n",
        "train_generator =train_datagen.flow_from_directory(train_dir,target_size=(224,224),batch_size=32)\n",
        "test_generator=test_datagen.flow_from_directory(test_dir,shuffle=True,target_size=(224,224),batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhlQnxs_SXv"
      },
      "source": [
        "#### **1.4. CNN Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSM1725Ey4Y"
      },
      "source": [
        "A Convolutional Neural Network (ConvNet) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7b4d1a",
        "outputId": "529127ab-bc72-4c61-9b58-0f37b4ef1965"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5),input_shape=input_shape,activation='relu',name=\"conv2d_1\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),name=\"max_pooling2d_1\"))\n",
        "model.add(Conv2D(32, (3, 3),activation='relu',name=\"conv2d_2\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"max_pooling2d_2\"))\n",
        "model.add(Conv2D(64, (3, 3),activation='relu',name=\"conv2d_3\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name=\"max_pooling2d_3\"))\n",
        "model.add(Flatten(name=\"flatten_1\"))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(num_classes,activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6daeef2d",
        "outputId": "cd745aa5-f15e-4f8e-e399-deae820d6710"
      },
      "outputs": [],
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       test_dir,\n",
        "                       target_size=(224, 224),\n",
        "                       batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yd5dldbH2xn"
      },
      "source": [
        "When compiling the model, we provide **objective function (loss)**, **optimization method (adam)** and **accuracy** that we will follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IXz69bROKNH"
      },
      "source": [
        "'fit()' parameters:\n",
        "* **train:** training data,\n",
        "* **validation_data:** validation set,\n",
        "* **shuffle:** change of location of data in each epoch,\n",
        "* **verbose:** to be able to see the outputs during the training (0-> does not show, 1-> does)\n",
        "* **epoch:** determines how many times the dataset will be trained by traversing the model\n",
        "* **callbacks:** An object that can perform actions at various stages of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d3b4edf",
        "outputId": "7fbbde72-52db-4fba-c48c-bafe4e7a30fb"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "history1 = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=32,\n",
        "    epochs=12,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=4,\n",
        "    verbose=1,\n",
        "    callbacks=[ReduceLROnPlateau(monitor='loss', factor=0.3,patience=3, min_lr=0.000001)],\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a616bb60",
        "outputId": "800bfe98-049c-4112-c094-b489324be69f"
      },
      "outputs": [],
      "source": [
        "model.save('/content/sample_data/CNN-model.h5')\n",
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model.evaluate(validation_generator)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnyAcVHQCmUG"
      },
      "source": [
        "#### **1.5. VGG16 Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_CAE0q1c4Tg"
      },
      "source": [
        "**VGG16** Architecture consists of 16 layers.\n",
        "\n",
        "* **include_top :** Whether to include 3 layers fully connected to the top of the network\n",
        "* **weight:** checkpoint from which model is initialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GND3gR2Cr9K",
        "outputId": "1c11d312-58ae-4e74-8394-887fa36bd381"
      },
      "outputs": [],
      "source": [
        "def create_Base_model_using_VGG16():\n",
        "    model = VGG16(\n",
        "        weights = \"imagenet\",\n",
        "        include_top=False,\n",
        "        input_shape = (224,224, 3) # goruntu boyutu\n",
        "        )\n",
        "    for layer in model.layers:\n",
        "      layer.trainable = False\n",
        "    return model\n",
        "create_Base_model_using_VGG16().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy5A0gDCC-2j",
        "outputId": "233ff3bd-bdd9-425a-ffda-53e5d1de894c"
      },
      "outputs": [],
      "source": [
        "def add_new_layers():\n",
        "    model = create_Base_model_using_VGG16()\n",
        "    x = model.output\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    predictions = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    final_model = tf.keras.models.Model(\n",
        "        inputs = model.input,\n",
        "        outputs = predictions)\n",
        "\n",
        "    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return final_model\n",
        "\n",
        "add_new_layers().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUqDVd65jRVQ",
        "outputId": "fe2a9265-c86f-47fd-e775-59e3bd2c3cb8"
      },
      "outputs": [],
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       test_dir, \n",
        "                       target_size=(224, 224),\n",
        "                       batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9u0Ny_CC-_H",
        "outputId": "0bf8adc8-ed44-47c9-f281-a2fc34fe5ac0"
      },
      "outputs": [],
      "source": [
        "model_from_vgg16 = add_new_layers()\n",
        "history2 = model_from_vgg16.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=32,\n",
        "    epochs=12,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=4,\n",
        "    verbose=1,\n",
        "    callbacks=[ReduceLROnPlateau(monitor='loss', factor=0.3,patience=3, min_lr=0.000001)],\n",
        "    use_multiprocessing=False,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93fUErbvC_C6"
      },
      "outputs": [],
      "source": [
        "model_from_vgg16.save('/content/sample_data/VGG16-model.h5')\n",
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model.evaluate(validation_generator)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9uxSFjxMY9w"
      },
      "source": [
        "#### **1.7. InceptionV3 Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MISclavSt04"
      },
      "source": [
        " Assigning the weight parameter to the imagenet will enable the weights of the imagenet model to be used. If we want to train something using the Inception mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76v_E9vjMeTp"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [224, 224]\n",
        "inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZfyn8-HMeXE",
        "outputId": "9c1600d1-bc1e-4362-9033-6f78bc07e890"
      },
      "outputs": [],
      "source": [
        "x = (inception.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Flatten()(Dense(64, activation='relu')(x))\n",
        "prediction = Dense(num_classes, activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=prediction)# Create a model object\n",
        "\n",
        "model.summary()# View the structure of the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRxW373rI0Q4",
        "outputId": "2d4b28d7-c3c8-42a1-bb49-f2d258200d96"
      },
      "outputs": [],
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       test_dir,\n",
        "                       target_size=(224, 224),\n",
        "                       batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_7g_QOKMyFH"
      },
      "outputs": [],
      "source": [
        "# Defining the cost and model optimization method to use\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history4 = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=32,\n",
        "    epochs=12,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=4,\n",
        "    verbose=1,\n",
        "    callbacks=[ReduceLROnPlateau(monitor='loss', factor=0.3,patience=3, min_lr=0.000001)],\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwdq-24FOFvT"
      },
      "outputs": [],
      "source": [
        "# Saving the model as a h5 file\n",
        "model.save('/content/sample_data/inception-model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgwxOoX0EQhl"
      },
      "source": [
        "#### **1.8. AlexNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xerOG713Kkc7"
      },
      "source": [
        "This deep convolutional neural network consisting of 25 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX_DgSpHScMN",
        "outputId": "e6e684a3-e5b2-4755-fc37-6a3f9cf5c7b7"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 4096, activation = 'relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 4096, activation = 'relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 1000, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = num_classes, activation = 'softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gljH5b7oQEU0",
        "outputId": "bcc4dddb-f07a-4339-e8be-8b3bb9ff34db"
      },
      "outputs": [],
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       test_dir,\n",
        "                       target_size=(224, 224),\n",
        "                       batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYV4cGQacYhn",
        "outputId": "49f4d94d-5cb3-4ad8-e32c-ca2449186ff1"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=32,\n",
        "    epochs=12,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=4,\n",
        "    verbose=1,\n",
        "    callbacks=[ReduceLROnPlateau(monitor='loss', factor=0.3,patience=3, min_lr=0.000001)],\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnPc-QEh0vTL"
      },
      "outputs": [],
      "source": [
        "model.save('/content/sample_data/Alexnet-model.h5')\n",
        "\n",
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model_alexnet.evaluate(validation_generator)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpotNoKAJqHc"
      },
      "outputs": [],
      "source": [
        "results = model_alexnet.evaluate(test_generator, verbose=1)\n",
        "\n",
        "print(\" Test Loss: {:.2f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMhog31hKNOl"
      },
      "outputs": [],
      "source": [
        "# Predict the label of the test\n",
        "pred = model.predict(test_generator,verbose=1)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "pred = [labels[k] for k in pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7TQRwLmJvZP"
      },
      "outputs": [],
      "source": [
        "y_test = list(train_generator.Label)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEIL5dcQIUU9"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "print(history5.history.keys())\n",
        "\n",
        "plt.plot(history5.history['accuracy'])\n",
        "plt.plot(history5.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history5.history['loss'])\n",
        "plt.plot(history5.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ogK7rSCKCj"
      },
      "source": [
        "#### **1.9. ResNet50 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUuLyWtfCKCl",
        "outputId": "c14973d3-615c-4eed-a262-d6d76103a4ae"
      },
      "outputs": [],
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "pretrained_model = ResNet50(\n",
        "    input_shape=(224,224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "pretrained_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgAlKJT74fL8"
      },
      "outputs": [],
      "source": [
        "inputs = pretrained_model.input\n",
        "\n",
        "x = Dense(64, activation='relu')(pretrained_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQBMDngK4jW2"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "my_callbacks  = [tf.keras.callbacks.EarlyStopping(monitor='loss',min_delta=0,patience=5,mode='auto')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMSdVG1KLhZm",
        "outputId": "0fcfd184-b541-44bb-e459-800e5d769274"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=12,\n",
        "    callbacks=my_callbacks,\n",
        "    verbose=1,\n",
        "    steps_per_epoch=32,\n",
        "    validation_steps=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrxc-tSGYShB"
      },
      "outputs": [],
      "source": [
        "model.save('/content/sample_data/model_Resnet50.h5')\n",
        "\n",
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model.evaluate(validation_generator)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDV-AlrZ2UlM",
        "outputId": "52287c64-ba80-4b45-c3d6-d6bf86b9edb7"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n",
        "plt.title(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame(history.history)[['loss','val_loss']].plot()\n",
        "plt.title(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YybBG0FDCCMQ"
      },
      "source": [
        "#### **Predict CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68VIb5WoAIou"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model_cnn=load_model('/content/sample_data/CNN-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "bc186b0d",
        "outputId": "480c8413-d16a-446f-a3c7-de419d0063dd"
      },
      "outputs": [],
      "source": [
        "classes=list(train_generator.class_indices.keys())\n",
        "def prepare(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = x/255\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "img_url='/content/Test/test.jpg'\n",
        "result_cnn = model_cnn.predict([prepare(img_url)])\n",
        "casting_img=image.load_img(img_url)\n",
        "plt.imshow(casting_img)\n",
        "\n",
        "classresult=np.argmax(result_cnn,axis=1)\n",
        "print(classes[classresult[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euXbHlTReseg"
      },
      "source": [
        "#### **Predict VGG16 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMjk-ShKesej"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model_vgg16=load_model('/content/sample_data/VGG16-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Cmjrf-Tt8BlK",
        "outputId": "4f6e6145-bc22-439b-9bc3-0eb35dc5f2c5"
      },
      "outputs": [],
      "source": [
        "classes=list(train_generator.class_indices.keys())\n",
        "def prepare(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = x/255\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "img_url='/content/Test/test.jpg'\n",
        "result_vgg16 = model_vgg16.predict([prepare(img_url)])\n",
        "casting_img=image.load_img(img_url)\n",
        "plt.imshow(casting_img)\n",
        "\n",
        "classresult=np.argmax(result_vgg16,axis=1)\n",
        "print(classes[classresult[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XluDawQiSBbc"
      },
      "source": [
        "#### **Predict Inception Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jQW-fvESHAu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model_inception=load_model('/content/sample_data/inception-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xcHJA-r77zKc",
        "outputId": "00a28e6a-49bd-43d2-c2b3-d18ae6087ddb"
      },
      "outputs": [],
      "source": [
        "classes=list(train_generator.class_indices.keys())\n",
        "def prepare(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = x/255\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "img_url='/content/Test/test.jpg'\n",
        "result_inception = model_inception.predict([prepare(img_url)])\n",
        "casting_img=image.load_img(img_url)\n",
        "plt.imshow(casting_img)\n",
        "\n",
        "classresult=np.argmax(result_inception,axis=1)\n",
        "print(classes[classresult[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq8erty27Ip2"
      },
      "source": [
        "#### **Predict AlexNet Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wf0Gmxe7Ip2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model_alexnet=load_model('/content/sample_data/Alexnet-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "0fvmPuln7Ip2",
        "outputId": "7c6eb8e6-364a-42f4-f5a8-74274bfe496e"
      },
      "outputs": [],
      "source": [
        "classes=list(train_generator.class_indices.keys())\n",
        "def prepare(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = x/255\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "img_url='/content/Test/test.jpg'\n",
        "result_alexnet = model_alexnet.predict([prepare(img_url)])\n",
        "casting_img=image.load_img(img_url)\n",
        "plt.imshow(casting_img)\n",
        "\n",
        "classresult=np.argmax(result_alexnet,axis=1)\n",
        "print(classes[classresult[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9_tKl-e1He"
      },
      "source": [
        "#### **Predict ResNet50 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL2dRYB8e1Hh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model_alexnet=load_model('/content/sample_data/model_Resnet50.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "2WlPXLWge1Hi",
        "outputId": "7c6eb8e6-364a-42f4-f5a8-74274bfe496e"
      },
      "outputs": [],
      "source": [
        "classes=list(train_generator.class_indices.keys())\n",
        "# Pre-Processing test data same as train data.\n",
        "def prepare(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = x/255\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "img_url='/content/Test/test.jpg'\n",
        "result_alexnet = model_alexnet.predict([prepare(img_url)])\n",
        "casting_img=image.load_img(img_url)\n",
        "plt.imshow(casting_img)\n",
        "\n",
        "classresult=np.argmax(result_alexnet,axis=1)\n",
        "print(classes[classresult[0]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L4uMifr__aDc",
        "C6tmATL4_ecG",
        "NuGK0vnQ_jRe",
        "EHhlQnxs_SXv",
        "EnyAcVHQCmUG",
        "CE9qVDQTFEct",
        "w9uxSFjxMY9w",
        "HgwxOoX0EQhl",
        "M_ogK7rSCKCj",
        "YybBG0FDCCMQ",
        "euXbHlTReseg",
        "wumWWhLB6gzc",
        "XluDawQiSBbc",
        "aq8erty27Ip2",
        "LS9_tKl-e1He"
      ],
      "gpuType": "T4",
      "name": "Plant_Disease_Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
